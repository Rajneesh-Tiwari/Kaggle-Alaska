{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alaska2 Baseline PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Ideas\n",
    "\n",
    "- 4 Classes\n",
    "- GroupKFold splitting\n",
    "- Class Balance\n",
    "- Flips\n",
    "- Label Smoothing\n",
    "- EfficientNetB2\n",
    "- ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch\n",
    "# !pip install albumentations\n",
    "# !pip install sklearn\n",
    "# !pip install pandas\n",
    "# !pip install -U catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from catalyst.data.sampler import BalanceClassSampler\n",
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"\n",
    "from catalyst.data.sampler import BalanceClassSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupKFold splitting\n",
    "\n",
    "I think group splitting by image_name is really important for correct validation in this competition ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for label, kind in enumerate(['Cover', 'JMiPOD', 'JUNIWARD', 'UERD']):\n",
    "    for path in glob('../input/data/Cover/*.jpg'):\n",
    "        dataset.append({\n",
    "            'kind': kind,\n",
    "            'image_name': path.split('/')[-1],\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "random.shuffle(dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "dataset.loc[:, 'fold'] = -1\n",
    "for fold_number, (train_index, val_index) in enumerate(gkf.split(X=dataset.index, y=dataset['label'], groups=dataset['image_name'])):\n",
    "    dataset.loc[dataset.iloc[val_index].index, 'fold'] = fold_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case original dataframe with splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>image_name</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>JMiPOD</td>\n",
       "      <td>44836.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>JMiPOD</td>\n",
       "      <td>56953.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>JMiPOD</td>\n",
       "      <td>73803.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>Cover</td>\n",
       "      <td>13999.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>Cover</td>\n",
       "      <td>62258.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          kind image_name  label  fold\n",
       "299995  JMiPOD  44836.jpg      1     0\n",
       "299996  JMiPOD  56953.jpg      1     3\n",
       "299997  JMiPOD  73803.jpg      1     4\n",
       "299998   Cover  13999.jpg      0     0\n",
       "299999   Cover  62258.jpg      0     4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv('../input/alaska2-public-baseline/groupkfold_by_shonenkov.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Augs: Flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = '../input/data'\n",
    "\n",
    "def onehot(size, target):\n",
    "    vec = torch.zeros(size, dtype=torch.float32)\n",
    "    vec[target] = 1.\n",
    "    return vec\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, kinds, image_names, labels, transforms=None):\n",
    "        super().__init__()\n",
    "        self.kinds = kinds\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "        target = onehot(4, label)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_number = 0\n",
    "\n",
    "# train_dataset = DatasetRetriever(\n",
    "#     kinds=dataset[dataset['fold'] != fold_number].kind.values,\n",
    "#     image_names=dataset[dataset['fold'] != fold_number].image_name.values,\n",
    "#     labels=dataset[dataset['fold'] != fold_number].label.values,\n",
    "#     transforms=get_train_transforms(),\n",
    "# )\n",
    "\n",
    "# validation_dataset = DatasetRetriever(\n",
    "#     kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "#     image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "#     labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "#     transforms=get_valid_transforms(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "def alaska_weighted_auc(y_true, y_valid):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/anokas/weighted-auc-metric-updated\n",
    "    \"\"\"\n",
    "    tpr_thresholds = [0.0, 0.4, 1.0]\n",
    "    weights = [2, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_valid, pos_label=1)\n",
    "\n",
    "    # size of subsets\n",
    "    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n",
    "\n",
    "    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n",
    "    normalization = np.dot(areas, weights)\n",
    "\n",
    "    competition_metric = 0\n",
    "    for idx, weight in enumerate(weights):\n",
    "        y_min = tpr_thresholds[idx]\n",
    "        y_max = tpr_thresholds[idx + 1]\n",
    "        mask = (y_min < tpr) & (tpr < y_max)\n",
    "        # pdb.set_trace()\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n",
    "\n",
    "        x = np.concatenate([fpr[mask], x_padding])\n",
    "        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n",
    "        y = y - y_min  # normalize such that curve starts at y=0\n",
    "        score = metrics.auc(x, y)\n",
    "        submetric = score * weight\n",
    "        best_subscore = (y_max - y_min) * weight\n",
    "        competition_metric += submetric\n",
    "\n",
    "    return competition_metric / normalization\n",
    "        \n",
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([0,1])\n",
    "        self.y_pred = np.array([0.5,0.5])\n",
    "        self.score = 0\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n",
    "        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        self.score = alaska_weighted_auc(self.y_true, self.y_pred)\n",
    "    \n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = 0.05):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            x = x.float()\n",
    "            target = target.float()\n",
    "            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n",
    "\n",
    "            nll_loss = -logprobs * target\n",
    "            nll_loss = nll_loss.sum(-1)\n",
    "    \n",
    "            smooth_loss = -logprobs.mean(dim=-1)\n",
    "\n",
    "            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path            \n",
    "\n",
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config,fold):\n",
    "        self.fold = fold\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.base_dir = '../models'\n",
    "        self.log_path = f'{self.base_dir}/{self.fold}/log.txt'\n",
    "\n",
    "        if not os.path.exists(f'{self.base_dir}/{self.fold}'):\n",
    "            os.mkdir(f'{self.base_dir}/{self.fold}')\n",
    "        if not os.path.exists(self.log_path):\n",
    "            open(self.log_path, 'w+').close()\n",
    "\n",
    "#         if not os.path.isfile(self.log_path):     \n",
    "#             filename = Path(self.log_path)\n",
    "#             filename.touch(exist_ok=True)\n",
    "\n",
    "        self.best_summary_loss = 10**5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.criterion = LabelSmoothing().to(self.device)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        \n",
    "        oName = f'{self.base_dir}/{self.fold}'\n",
    "        if not os.path.exists(oName):\n",
    "            os.makedirs(oName)\n",
    "            \n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.train_one_epoch(train_loader)\n",
    "            \n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/{self.fold}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.validation(validation_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/{self.fold}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/{self.fold}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                targets = targets.to(self.device).float()\n",
    "                batch_size = images.shape[0]\n",
    "                images = images.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                final_scores.update(targets, outputs)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "            targets = targets.to(self.device).float()\n",
    "            images = images.to(self.device).float()\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            final_scores.update(targets, outputs)\n",
    "            summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)                        \n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def get_net(arch):\n",
    "    net = EfficientNet.from_pretrained(arch)\n",
    "    in_features  = net._fc.in_features\n",
    "    net._fc = nn.Linear(in_features=in_features, out_features=4, bias=True)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 4\n",
    "    batch_size = 8\n",
    "    n_epochs = 35\n",
    "    lr = 0.001\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = True  # do scheduler.step after validation stage loss\n",
    "    \n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        verbose=False, \n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Balance \"on fly\" from [@CatalystTeam](https://github.com/catalyst-team/catalyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>image_name</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JUNIWARD</td>\n",
       "      <td>34730.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cover</td>\n",
       "      <td>30243.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JUNIWARD</td>\n",
       "      <td>24096.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JMiPOD</td>\n",
       "      <td>07345.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JMiPOD</td>\n",
       "      <td>38183.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       kind image_name  label  fold\n",
       "0  JUNIWARD  34730.jpg      2     3\n",
       "1     Cover  30243.jpg      0     1\n",
       "2  JUNIWARD  24096.jpg      2     0\n",
       "3    JMiPOD  07345.jpg      1     0\n",
       "4    JMiPOD  38183.jpg      1     0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.reset_index(drop=True,inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _ = dataset[dataset['fold'] != 0].reset_index(drop=True)\n",
    "# _.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "def run_training(fold,arch='efficientnet-b6'):\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "#     _ = dataset.reset_index()\n",
    "    fold_number = fold\n",
    "\n",
    "    a1 = dataset[dataset['fold'] != fold_number].reset_index(drop=True)\n",
    "    a2 = dataset[dataset['fold'] == fold_number].reset_index(drop=True)\n",
    "        \n",
    "    train_dataset = DatasetRetriever(\n",
    "        kinds=a1.kind.values,\n",
    "        image_names=a1.image_name.values,\n",
    "        labels=a1.label.values,\n",
    "        transforms=get_train_transforms(),\n",
    "    )\n",
    "\n",
    "    validation_dataset = DatasetRetriever(\n",
    "        kinds=a2.kind.values,\n",
    "        image_names=a2.image_name.values,\n",
    "        labels=a2.label.values,\n",
    "        transforms=get_valid_transforms(),\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    net = get_net(arch).cuda()\n",
    "    \n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig,fold=fold_number)\n",
    "    fitter.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have used 1xV100 for training model, in kaggle kernel it works also. You can make fork and check it, but I would like to share with you my logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "Fitter prepared. Device is cuda:0\n",
      "\n",
      "2020-07-11T17:38:35.467340\n",
      "LR: 0.001\n",
      "Train Step 4653/30000, summary_loss: 1.32882, final_score: 0.64073, time: 3120.55807\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step 5965/30000, summary_loss: 1.31034, final_score: 0.65301, time: 4006.31901\r"
     ]
    }
   ],
   "source": [
    "arch = 'efficientnet-b4'\n",
    "run_training(0,arch)\n",
    "run_training(1,arch)\n",
    "run_training(2,arch)\n",
    "run_training(3,arch)\n",
    "run_training(4,arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# file = open('../input/alaska2-public-baseline/log.txt', 'r')\n",
    "# for line in file.readlines():\n",
    "#     print(line[:-1])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('../input/alaska2-public-baseline/best-checkpoint-033epoch.bin')\n",
    "# net.load_state_dict(checkpoint['model_state_dict']);\n",
    "# net.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In checkpoint you can find states for optimizer and scheduler if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DatasetSubmissionRetriever(Dataset):\n",
    "\n",
    "#     def __init__(self, image_names, transforms=None):\n",
    "#         super().__init__()\n",
    "#         self.image_names = image_names\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __getitem__(self, index: int):\n",
    "#         image_name = self.image_names[index]\n",
    "#         image = cv2.imread(f'{DATA_ROOT_PATH}/Test/{image_name}', cv2.IMREAD_COLOR)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "#         image /= 255.0\n",
    "#         if self.transforms:\n",
    "#             sample = {'image': image}\n",
    "#             sample = self.transforms(**sample)\n",
    "#             image = sample['image']\n",
    "\n",
    "#         return image_name, image\n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return self.image_names.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = DatasetSubmissionRetriever(\n",
    "#     image_names=np.array([path.split('/')[-1] for path in glob('../input/alaska2-image-steganalysis/Test/*.jpg')]),\n",
    "#     transforms=get_valid_transforms(),\n",
    "# )\n",
    "\n",
    "\n",
    "# data_loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2,\n",
    "#     drop_last=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# result = {'Id': [], 'Label': []}\n",
    "# for step, (image_names, images) in enumerate(data_loader):\n",
    "#     print(step, end='\\r')\n",
    "    \n",
    "#     y_pred = net(images.cuda())\n",
    "#     y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "    \n",
    "#     result['Id'].extend(image_names)\n",
    "#     result['Label'].extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame(result)\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission['Label'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for reading my kernel!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02a9428e86e648278863828c8415ba06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e3894c960204a5b9945ed321e529f72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5e5ee2bb939d4d5592a295db929297c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6115b6738a03430d82118c6871c2793f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_02a9428e86e648278863828c8415ba06",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_5e5ee2bb939d4d5592a295db929297c0",
       "value": " 35.1M/35.1M [00:06&lt;00:00, 5.43MB/s]"
      }
     },
     "6d75db7ce15e4a0cb47bec8af2c84925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ca49c5d7180f4d64ba9756aeb03c7e5f",
       "max": 36804509,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5e3894c960204a5b9945ed321e529f72",
       "value": 36804509
      }
     },
     "9806b1a81207410d85cd625c591c04e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca49c5d7180f4d64ba9756aeb03c7e5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3bce2515e7f401580708eb86cf3cacb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6d75db7ce15e4a0cb47bec8af2c84925",
        "IPY_MODEL_6115b6738a03430d82118c6871c2793f"
       ],
       "layout": "IPY_MODEL_9806b1a81207410d85cd625c591c04e1"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
